{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\r\n",
      "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 5.5 MB 3.3 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\r\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-macosx_10_11_x86_64.whl (3.8 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 3.8 MB 126.9 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.17 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.3.15)\r\n",
      "Requirement already satisfied: requests in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.27.1)\r\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\r\n",
      "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 182 kB 136.2 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: filelock in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\r\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\r\n",
      "Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.24.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\r\n",
      "  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 451 kB 2.4 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: tqdm>=4.62.1 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from datasets) (4.64.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from datasets) (0.11.0)\r\n",
      "Collecting multiprocess\r\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 132 kB 45.3 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting dill<0.3.7\r\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 110 kB 70.0 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: pandas in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from datasets) (1.4.2)\r\n",
      "Collecting responses<0.19\r\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\r\n",
      "Requirement already satisfied: packaging in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from datasets) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from datasets) (6.0)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2.27.1)\r\n",
      "Collecting xxhash\r\n",
      "  Downloading xxhash-3.1.0-cp39-cp39-macosx_10_9_x86_64.whl (34 kB)\r\n",
      "Collecting pyarrow>=6.0.0\r\n",
      "  Downloading pyarrow-10.0.1-cp39-cp39-macosx_10_14_x86_64.whl (25.1 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 25.1 MB 71.5 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: aiohttp in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.1)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from datasets) (2022.2.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from datasets) (1.21.5)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.1.1)\r\n",
      "Requirement already satisfied: filelock in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.6.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from packaging->datasets) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (5.2.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.6.3)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\r\n",
      "Installing collected packages: dill, xxhash, responses, pyarrow, multiprocess, datasets\r\n",
      "Successfully installed datasets-2.7.1 dill-0.3.6 multiprocess-0.70.14 pyarrow-10.0.1 responses-0.18.0 xxhash-3.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizerFast\n",
    "import json\n",
    "import os\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset bookcorpus (/Users/gauravsharma/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/eddee3cae1cc263a431aa98207d4d27fd8a73b0a9742f692af0e6c65afa4d75f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4721c4cad0fe4fa3b784e70359396796"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"bookcorpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /Users/gauravsharma/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/eddee3cae1cc263a431aa98207d4d27fd8a73b0a9742f692af0e6c65afa4d75f/cache-6906a9172deb1853.arrow and /Users/gauravsharma/.cache/huggingface/datasets/bookcorpus/plain_text/1.0.0/eddee3cae1cc263a431aa98207d4d27fd8a73b0a9742f692af0e6c65afa4d75f/cache-addc1b2dc2e4cd23.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset[\"train\"]\n",
    "dataset = dataset.train_test_split(test_size=0.001, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 73930223\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 74005\n    })\n})"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"train\"][\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in dataset[\"test\"][\"text\"][:3]:\n",
    "#   print(t)\n",
    "#   print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
    "  \"\"\"Utility function to save dataset text to disk,\n",
    "  useful for using the texts to train the tokenizer\n",
    "  (as the tokenizer accepts files)\"\"\"\n",
    "  with open(output_filename, \"w\") as f:\n",
    "    for t in dataset[\"text\"]:\n",
    "      print(t, file=f)\n",
    "\n",
    "# save the training set to train.txt\n",
    "dataset_to_text(dataset[\"train\"], \"/Users/gauravsharma/PycharmProjects/optimized-bert/data/raw/train.txt\")\n",
    "# save the testing set to test.txt\n",
    "dataset_to_text(dataset[\"test\"], \"/Users/gauravsharma/PycharmProjects/optimized-bert/data/raw/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "  \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n",
    "]\n",
    "# if you want to train the tokenizer on both sets\n",
    "# files = [\"train.txt\", \"test.txt\"]\n",
    "# training the tokenizer on the training set\n",
    "files = [\"/Users/gauravsharma/PycharmProjects/optimized-bert/data/raw/test.txt\"]\n",
    "# 30,522 vocab is BERT's default vocab size, feel free to tweak\n",
    "vocab_size = 30_522\n",
    "# maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
    "max_length = 512\n",
    "# whether to truncate\n",
    "truncate_longer_samples = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize the WordPiece tokenizer\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "# train the tokenizer\n",
    "tokenizer.train(files=files, vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "# enable truncation up to the maximum 512 tokens\n",
    "tokenizer.enable_truncation(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"/Users/gauravsharma/PycharmProjects/optimized-bert/models/tokenizer\"\n",
    "\n",
    "if not os.path.isdir(tokenizer_path):\n",
    "  os.mkdir(tokenizer_path)\n",
    "# save the tokenizer\n",
    "tokenizer.save_model(tokenizer_path)\n",
    "# dumping some of the tokenizer config to config file,\n",
    "# including special tokens, whether to lower case and the maximum sequence length\n",
    "with open(os.path.join(tokenizer_path, \"config.json\"), \"w\") as f:\n",
    "  tokenizer_cfg = {\n",
    "      \"do_lower_case\": True,\n",
    "      \"unk_token\": \"[UNK]\",\n",
    "      \"sep_token\": \"[SEP]\",\n",
    "      \"pad_token\": \"[PAD]\",\n",
    "      \"cls_token\": \"[CLS]\",\n",
    "      \"mask_token\": \"[MASK]\",\n",
    "      \"model_max_length\": max_length,\n",
    "      \"max_len\": max_length,\n",
    "  }\n",
    "  json.dump(tokenizer_cfg, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_path = \"/Users/gauravsharma/PycharmProjects/optimized-bert/models/tokenizer\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/75 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2fba2ba8db864603be71ddbc26f205be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def encode_with_truncation(examples):\n",
    "  \"\"\"Mapping function to tokenize the sentences passed with truncation\"\"\"\n",
    "  return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\",\n",
    "                   max_length=max_length, return_special_tokens_mask=True)\n",
    "\n",
    "def encode_without_truncation(examples):\n",
    "  \"\"\"Mapping function to tokenize the sentences passed without truncation\"\"\"\n",
    "  return tokenizer(examples[\"text\"], return_special_tokens_mask=True)\n",
    "\n",
    "# the encode function will depend on the truncate_longer_samples variable\n",
    "encode = encode_with_truncation if truncate_longer_samples else encode_without_truncation\n",
    "# tokenizing the train dataset\n",
    "# train_dataset = dataset[\"train\"].map(encode, batched=True)\n",
    "# tokenizing the testing dataset\n",
    "test_dataset = dataset[\"test\"].map(encode, batched=True)\n",
    "if truncate_longer_samples:\n",
    "  # remove other columns and set input_ids and attention_mask as PyTorch tensors\n",
    "  # train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "  test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "else:\n",
    "  # remove other columns, and remain them as Python lists\n",
    "  test_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "  # train_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Grouping texts in chunks of 512:   0%|          | 0/75 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d38e3b80b4bc418fa274596f51b8e63c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# max_seq_length.\n",
    "# grabbed from: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= max_length:\n",
    "        total_length = (total_length // max_length) * max_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n",
    "# remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n",
    "# might be slower to preprocess.\n",
    "#\n",
    "# To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "# https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "if not truncate_longer_samples:\n",
    "  # train_dataset = train_dataset.map(group_texts, batched=True,\n",
    "  #                                   desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "  test_dataset = test_dataset.map(group_texts, batched=True,\n",
    "                                  desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "  # convert them from lists to torch tensors\n",
    "\n",
    "  # train_dataset.set_format(\"torch\")\n",
    "  test_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2353"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM\n",
    "from transformers import TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM(config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "BertForMaskedLM(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (cls): BertOnlyMLMHead(\n    (predictions): BertLMPredictionHead(\n      (transform): BertPredictionHeadTransform(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (transform_act_fn): GELUActivation()\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n    )\n  )\n)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "model_path = \"/Users/gauravsharma/PycharmProjects/optimized-bert/models/exp01/model\"\n",
    "\n",
    "if not os.path.isdir(model_path):\n",
    "  os.mkdir(model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,          # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,            # number of training epochs, feel free to tweak\n",
    "    per_device_train_batch_size=10, # the training batch size, put it as high as your GPU memory fits\n",
    "    gradient_accumulation_steps=8,  # accumulating the gradients before updating the weights\n",
    "    per_device_eval_batch_size=64,  # evaluation batch size\n",
    "    logging_steps=1000,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=1000,\n",
    "    # load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
    "    # save_total_limit=3,           # whether you don't have much space so you let only 3 model weights saved in the disk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the trainer and pass everything to it\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=test_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/Users/gauravsharma/opt/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2353\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 80\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 29\n",
      "  Number of trainable parameters = 109514298\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
